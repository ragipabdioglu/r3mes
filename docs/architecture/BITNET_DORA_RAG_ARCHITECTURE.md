# R3MES AI Architecture: BitNet + DoRA + RAG

> **Version:** 3.4 (Integration Tests TamamlandÄ±)
> **Son GÃ¼ncelleme:** Ocak 2026
> **Implementation Status:** 14/14 temel bileÅŸen tamamlandÄ± (215 test geÃ§ti)

## Genel BakÄ±ÅŸ

R3MES, Ã¼Ã§ temel teknoloji Ã¼zerine kurulu merkezi olmayan bir AI sistemidir:

1. **BitNet** - 1.58-bit quantized base model (frozen backbone)
2. **DoRA** - Weight-Decomposed Low-Rank Adaptation (trainable experts)
3. **RAG** - Retrieval-Augmented Generation (gÃ¼ncel bilgi eriÅŸimi)

## Tamamlanan BileÅŸenler

| # | BileÅŸen | Dosya | Test SayÄ±sÄ± | Durum |
|---|---------|-------|-------------|-------|
| 1 | BitLinear | `core/bitlinear.py` | - | âœ… |
| 2 | DoRA Layer | `core/dora.py` | 19 | âœ… |
| 3 | Inference Backend | `core/inference_backend.py` | 15 | âœ… |
| 4 | PyTorch Backend | `core/backends/pytorch_backend.py` | - | âœ… |
| 5 | Tiered Cache | `cache/tiered_cache.py` | 19 | âœ… |
| 6 | VRAM Manager | `cache/vram_manager.py` | - | âœ… |
| 7 | Keyword Router | `router/keyword_router.py` | 22 | âœ… |
| 8 | Semantic Router | `router/semantic_router.py` | 19 | âœ… |
| 9 | Hybrid Router | `router/hybrid_router.py` | 19 | âœ… |
| 10 | VRAM Adaptive Gating | `router/vram_adaptive_gating.py` | - | âœ… |
| 11 | FAISS Store | `rag/faiss_store.py` | 17 | âœ… |
| 12 | RAG Embedder | `rag/embedder.py` | 15 | âœ… |
| 13 | RAG Retriever | `rag/retriever.py` | 17 | âœ… |
| 14 | **Inference Pipeline** | `r3mes/serving/inference_pipeline.py` | 27 | âœ… |
| 15 | **Integration Tests** | `tests/test_integration_pipeline.py` | 26 | âœ… **YENÄ°** |

**Toplam: 215 test geÃ§ti âœ…**

---

## TasarÄ±m KararlarÄ± (Design Decisions)

Bu bÃ¶lÃ¼m, mimari tartÄ±ÅŸmalar sonucu alÄ±nan kritik kararlarÄ± iÃ§erir.

### Karar 1: Hybrid Router Strategy (Keyword + Semantic + VRAM-Adaptive)

**Problem:** 
- Sadece keyword router: HÄ±zlÄ± ama edge case'lerde zayÄ±f
- Sadece semantic router: DoÄŸru ama yavaÅŸ (~10-20ms)
- Multi-adapter inference VRAM'i ÅŸiÅŸirebilir

**Ã‡Ã¶zÃ¼m:** 4 aÅŸamalÄ± Hybrid Router Pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         HYBRID ROUTER PIPELINE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  INPUT: User Query                                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  STAGE 1: KEYWORD ROUTER (Fast Pre-filter)                          â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Latency: <1ms                                                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Method: Regex patterns, domain/language/task detection         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Output: candidate_experts + confidence                         â”‚   â”‚
â”‚  â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”‚  IF confidence >= 0.85 â†’ SKIP Stage 2 (fast path) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  ELSE â†’ Continue to Stage 2                                   â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                               â”‚   â”‚
â”‚         â–¼ (confidence < 0.85)                                           â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  STAGE 2: SEMANTIC ROUTER (Deep Understanding)                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Latency: ~10-15ms                                              â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Model: all-MiniLM-L6-v2 (22M params, 384 dim)                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Method: Query embedding â†’ Cosine sim with expert embeddings    â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Expert Embeddings: 15-20 representative queries per expert     â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Similarity: Max pooling over expert embeddings                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                               â”‚   â”‚
â”‚         â–¼                                                               â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  STAGE 3: SCORE FUSION                                              â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Keyword weight: 0.3                                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Semantic weight: 0.7                                           â”‚   â”‚
â”‚  â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”‚  final_score = 0.3 Ã— keyword_score + 0.7 Ã— semantic_score        â”‚   â”‚
â”‚  â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”‚  Adaptive (optional):                                            â”‚   â”‚
â”‚  â”‚  â”‚  - Keyword confidence yÃ¼ksekse â†’ keyword weight artÄ±r            â”‚   â”‚
â”‚  â”‚  â”‚  - Ambiguous query â†’ semantic weight artÄ±r                       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                               â”‚   â”‚
â”‚         â–¼                                                               â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  STAGE 4: VRAM-ADAPTIVE GATING                                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ VRAM < 8GB  â†’ Top-1 expert                                     â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ VRAM 8-16GB â†’ Top-2 experts                                    â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ VRAM > 16GB â†’ Top-3 experts                                    â”‚   â”‚
â”‚  â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”‚  Fallback: general_dora if max_score < 0.5                       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚  OUTPUT: [(expert_id, weight), ...]                                         â”‚
â”‚          Example: [("medical_dora", 0.6), ("turkish_dora", 0.4)]           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Neden Hybrid?**
| YaklaÅŸÄ±m | Latency | Accuracy | Edge Cases | Resource |
|----------|---------|----------|------------|----------|
| Keyword Only | <1ms | DÃ¼ÅŸÃ¼k-Orta | ZayÄ±f | Minimal |
| Semantic Only | ~15ms | YÃ¼ksek | Ä°yi | Model yÃ¼kleme |
| **Hybrid** | **~5-10ms** | **En YÃ¼ksek** | **Ã‡ok Ä°yi** | **Orta** |

**Fast Path Optimization:**
- Keyword confidence >= 0.85 â†’ Semantic router atlanÄ±r
- Ã–rnek: "Python'da for loop nasÄ±l yazÄ±lÄ±r?" â†’ coding_dora (0.95) â†’ Fast path
- Ã–rnek: "Bu konuyu aÃ§Ä±klar mÄ±sÄ±n?" â†’ (0.3) â†’ Semantic router Ã§alÄ±ÅŸÄ±r

**Uygulama:**
```python
class HybridRouter:
    def __init__(self):
        self.keyword_router = KeywordRouter()
        self.semantic_router = SemanticRouter()
        self.gating = VRAMAdaptiveGating()
        
        # Weights
        self.keyword_weight = 0.3
        self.semantic_weight = 0.7
        self.fast_path_threshold = 0.85
    
    def route(self, query: str, vram_gb: float) -> List[Tuple[str, float]]:
        # Stage 1: Keyword Router
        keyword_results = self.keyword_router.route(query)
        max_keyword_conf = max((r.confidence for r in keyword_results), default=0)
        
        # Fast path: Skip semantic if keyword is confident enough
        if max_keyword_conf >= self.fast_path_threshold:
            scores = [(r.expert_id, r.confidence) for r in keyword_results]
        else:
            # Stage 2: Semantic Router
            semantic_results = self.semantic_router.route(query)
            
            # Stage 3: Score Fusion
            scores = self._fuse_scores(keyword_results, semantic_results)
        
        # Stage 4: VRAM-Adaptive Gating
        return self.gating.select(scores, vram_gb)
    
    def _fuse_scores(self, keyword, semantic) -> List[Tuple[str, float]]:
        combined = {}
        for r in keyword:
            combined[r.expert_id] = self.keyword_weight * r.confidence
        for r in semantic:
            if r.expert_id in combined:
                combined[r.expert_id] += self.semantic_weight * r.score
            else:
                combined[r.expert_id] = self.semantic_weight * r.score
        return sorted(combined.items(), key=lambda x: x[1], reverse=True)
```


---

### Karar 2: Tiered Caching - Cold Start Ã‡Ã¶zÃ¼mÃ¼

**Problem:** Diskten DoRA adapter yÃ¼klemek (cold start) latency'yi artÄ±rÄ±r.

**Ã‡Ã¶zÃ¼m:** 3 katmanlÄ± cache sistemi + predictive loading:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TIERED CACHING SYSTEM                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  TIER 1: VRAM (Hot Cache)                                           â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Latency: 0ms (zaten yÃ¼klÃ¼)                                     â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Capacity: 2-4 adapter (VRAM'e baÄŸlÄ±)                           â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Contents: turkish_dora, general_dora (her zaman)               â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Policy: Startup'ta preload                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                               â”‚
â”‚                             â–¼ (cache miss)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  TIER 2: RAM (Warm Cache)                                           â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Latency: ~5ms (memory copy)                                    â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Capacity: 10-20 adapter                                        â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Contents: SÄ±k kullanÄ±lan (medical, coding, legal)              â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Policy: LRU eviction                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                               â”‚
â”‚                             â–¼ (cache miss)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  TIER 3: DISK (Cold Cache)                                          â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Latency: ~50-100ms (disk I/O)                                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Capacity: Unlimited                                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Contents: Nadir kullanÄ±lan (cobol_dora, sanskrit_dora)         â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Policy: IPFS'ten indir, local cache                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  PREDICTIVE LOADING:                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Router "medical" dedi â†’ Inference baÅŸlarken arka planda:           â”‚   â”‚
â”‚  â”‚  asyncio.create_task(preload_to_vram("medical_dora"))               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Sonraki sorgu muhtemelen aynÄ± domain'den gelecek.                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Uygulama:**
```python
class TieredDoRACache:
    def __init__(self, vram_capacity_mb: int):
        self.vram_cache = {}      # Tier 1: GPU memory
        self.ram_cache = {}       # Tier 2: CPU memory  
        self.disk_cache_dir = ".r3mes/dora_cache"  # Tier 3
        
        # Startup preload
        self._preload_hot_adapters(["turkish_dora", "general_dora"])
    
    async def get_adapter(self, adapter_id: str) -> DoRAAdapter:
        # Tier 1: VRAM
        if adapter_id in self.vram_cache:
            return self.vram_cache[adapter_id]
        
        # Tier 2: RAM
        if adapter_id in self.ram_cache:
            adapter = self.ram_cache[adapter_id]
            await self._promote_to_vram(adapter_id, adapter)
            return adapter
        
        # Tier 3: Disk
        adapter = await self._load_from_disk(adapter_id)
        self.ram_cache[adapter_id] = adapter
        return adapter
    
    async def predictive_load(self, likely_adapters: List[str]):
        """Router sonucuna gÃ¶re arka planda yÃ¼kle."""
        for adapter_id in likely_adapters:
            if adapter_id not in self.vram_cache:
                asyncio.create_task(self._warm_up(adapter_id))
```

---

### Karar 3: Inference Backend Abstraction (Phased Approach)

**Problem:** BitNet 1.58-bit iÃ§in native optimized inference yok.

**Ã‡Ã¶zÃ¼m:** Abstract backend interface + phased implementation:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INFERENCE ENGINE ABSTRACTION                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    InferenceBackend (Abstract)                       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  interface:                                                         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ load_base_model(ipfs_hash) -> Model                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ load_dora_adapter(adapter_id) -> DoRAAdapter                   â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ inference(input, adapters, weights) -> Tensor                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ get_capabilities() -> {vram, speed, precision}                 â”‚   â”‚
â”‚  â”‚  â””â”€â”€ supports_feature(feature) -> bool                              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚         â”‚                   â”‚                   â”‚                          â”‚
â”‚         â–¼                   â–¼                   â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  PyTorch    â”‚    â”‚  Triton     â”‚    â”‚  BitNet-cpp â”‚                     â”‚
â”‚  â”‚  Backend    â”‚    â”‚  Backend    â”‚    â”‚  Backend    â”‚                     â”‚
â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚                     â”‚
â”‚  â”‚ âœ… Phase 1  â”‚    â”‚ ğŸ”„ Phase 2  â”‚    â”‚ ğŸ“… Phase 3  â”‚                     â”‚
â”‚  â”‚ (Åimdi)     â”‚    â”‚ (3-6 ay)    â”‚    â”‚ (6-12 ay)   â”‚                     â”‚
â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚                     â”‚
â”‚  â”‚ Features:   â”‚    â”‚ Features:   â”‚    â”‚ Features:   â”‚                     â”‚
â”‚  â”‚ â€¢ Kolay dev â”‚    â”‚ â€¢ Custom    â”‚    â”‚ â€¢ Native    â”‚                     â”‚
â”‚  â”‚ â€¢ Debug     â”‚    â”‚   kernels   â”‚    â”‚   1.58-bit  â”‚                     â”‚
â”‚  â”‚ â€¢ Fallback  â”‚    â”‚ â€¢ 2-3x hÄ±z  â”‚    â”‚ â€¢ 5-10x hÄ±z â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                             â”‚
â”‚  AUTO-SELECTION:                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  def get_best_backend() -> InferenceBackend:                        â”‚   â”‚
â”‚  â”‚      if triton_available() and has_nvidia_gpu():                    â”‚   â”‚
â”‚  â”‚          return TritonBackend()                                     â”‚   â”‚
â”‚  â”‚      elif bitnet_cpp_available():                                   â”‚   â”‚
â”‚  â”‚          return BitNetCppBackend()                                  â”‚   â”‚
â”‚  â”‚      else:                                                          â”‚   â”‚
â”‚  â”‚          return PyTorchBackend()  # Always available                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Phase 1 (Åimdi):** PyTorch Backend
- TÃ¼m mimariyi kur (DoRA, Router, RAG)
- End-to-end Ã§alÄ±ÅŸan sistem
- Development ve testing iÃ§in ideal

**Phase 2 (3-6 ay):** Triton Kernels
- BitLinear + DoRA iÃ§in custom kernels
- 2-3x performans artÄ±ÅŸÄ±
- PyTorch backend fallback olarak kalÄ±r

**Phase 3 (6-12 ay):** BitNet-cpp veya Custom C++
- Native 1.58-bit support
- 5-10x performans artÄ±ÅŸÄ±
- Production-grade optimization


---

### Karar 4: Custom DoRA Layer (BitLinear Entegrasyonu)

**Problem:** PEFT kÃ¼tÃ¼phanesi `nn.Linear` bekliyor, bizim `BitLinear` custom layer.

**Ã‡Ã¶zÃ¼m:** Custom DoRA layer yazacaÄŸÄ±z (PEFT wrapper yerine):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CUSTOM DoRA LAYER DESIGN                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  DoRA Formula:                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  output = Wâ‚€x + m * (V / ||V||) * x                                 â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Burada:                                                            â”‚   â”‚
â”‚  â”‚  â€¢ Wâ‚€ = BitLinear backbone (frozen, {-1, 0, +1})                    â”‚   â”‚
â”‚  â”‚  â€¢ m  = magnitude (learnable scalar per output dim)                 â”‚   â”‚
â”‚  â”‚  â€¢ V  = direction matrix = B @ A (low-rank)                         â”‚   â”‚
â”‚  â”‚  â€¢ ||V|| = column-wise L2 norm                                      â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Class Structure:                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  class BitLinearDoRA(nn.Module):                                    â”‚   â”‚
â”‚  â”‚      def __init__(self, bitlinear: BitLinear, rank: int = 16):      â”‚   â”‚
â”‚  â”‚          self.backbone = bitlinear           # Frozen                â”‚   â”‚
â”‚  â”‚          self.backbone.requires_grad_(False)                        â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚          # DoRA components (trainable)                              â”‚   â”‚
â”‚  â”‚          self.magnitude = nn.Parameter(                             â”‚   â”‚
â”‚  â”‚              torch.ones(bitlinear.out_features)                     â”‚   â”‚
â”‚  â”‚          )                                                          â”‚   â”‚
â”‚  â”‚          self.direction_A = nn.Parameter(                           â”‚   â”‚
â”‚  â”‚              torch.randn(rank, bitlinear.in_features) * 0.01        â”‚   â”‚
â”‚  â”‚          )                                                          â”‚   â”‚
â”‚  â”‚          self.direction_B = nn.Parameter(                           â”‚   â”‚
â”‚  â”‚              torch.zeros(bitlinear.out_features, rank)              â”‚   â”‚
â”‚  â”‚          )                                                          â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚      def forward(self, x: Tensor) -> Tensor:                        â”‚   â”‚
â”‚  â”‚          # Backbone (frozen BitNet)                                 â”‚   â”‚
â”‚  â”‚          backbone_out = self.backbone(x)                            â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚          # Direction: V = B @ A                                     â”‚   â”‚
â”‚  â”‚          V = self.direction_B @ self.direction_A  # [out, in]       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚          # Normalize direction (column-wise)                        â”‚   â”‚
â”‚  â”‚          V_norm = V / (V.norm(dim=1, keepdim=True) + 1e-8)          â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚          # DoRA output: m * normalized_direction * x                â”‚   â”‚
â”‚  â”‚          dora_out = self.magnitude.unsqueeze(0) * F.linear(x, V_norm)â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚          return backbone_out + dora_out                             â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Neden PEFT Wrapper DeÄŸil?                                                 â”‚
â”‚  â”œâ”€â”€ Tam kontrol: BitLinear'Ä±n quantized weights'i ile uyumlu             â”‚
â”‚  â”œâ”€â”€ Performans: Gereksiz abstraction yok                                  â”‚
â”‚  â”œâ”€â”€ Debug: Kolay inspect ve modify                                        â”‚
â”‚  â””â”€â”€ Future-proof: Triton kernels iÃ§in hazÄ±r                               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Trainable vs Frozen Parameters:**
```python
# Model parametreleri
Total Parameters:
â”œâ”€â”€ BitLinear backbone: ~1B params (FROZEN, {-1,0,+1})
â””â”€â”€ DoRA adapters: ~10-50M params (TRAINABLE)
    â”œâ”€â”€ magnitude: out_features per layer
    â”œâ”€â”€ direction_A: rank Ã— in_features per layer
    â””â”€â”€ direction_B: out_features Ã— rank per layer

# Ã–rnek (rank=16, hidden=4096):
# Per layer: 4096 + (16Ã—4096) + (4096Ã—16) = 4096 + 65536 + 65536 = 135,168 params
# 32 layer: 32 Ã— 135,168 = 4.3M trainable params
```

---

### Karar 5: Hybrid RAG Architecture (Merkezi + Yerel)

**Problem:** RAG tamamen merkezi mi yoksa daÄŸÄ±tÄ±k mÄ± olmalÄ±?

**Ã‡Ã¶zÃ¼m:** Hibrit mimari - Serving Node merkezi, Miner Node opsiyonel yerel:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       HYBRID RAG ARCHITECTURE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    SERVING NODE (Merkezi)                            â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Responsibilities:                                                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Global indices (news, general, medical, code)                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ SÄ±k gÃ¼ncellenen veriler (news: her 6 saat)                     â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ API endpoint saÄŸlar (/api/v1/rag/search)                       â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Index versioning ve sync                                       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Vector Store: FAISS (tercih edilen)                                â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ HÄ±zlÄ± (C++ backend)                                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Az dependency                                                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ IPFS'e serialize edilebilir                                    â”‚   â”‚
â”‚  â”‚  â””â”€â”€ GPU acceleration (IVF-PQ)                                      â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Storage:                                                           â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Index files: Local SSD                                         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Documents: IPFS (content-addressed)                            â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Embeddings: Numpy arrays (memory-mapped)                       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                               â”‚
â”‚                             â”‚ API calls                                     â”‚
â”‚                             â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    MINER NODE (Yerel - Opsiyonel)                    â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Use Cases:                                                         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Kendi training data'sÄ± iÃ§in mini index                         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Offline Ã§alÄ±ÅŸabilme (internet kesintisi)                       â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Privacy-sensitive data (ÅŸirket iÃ§i dokÃ¼manlar)                 â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Low-latency local retrieval                                    â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Sync Strategy:                                                     â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Serving Node'dan index snapshot indir (IPFS)                   â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Delta updates (sadece deÄŸiÅŸenler)                              â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Offline mode: Local cache kullan                               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                               â”‚
â”‚                             â”‚ IPFS sync                                     â”‚
â”‚                             â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    IPFS (DaÄŸÄ±tÄ±k Storage)                            â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Stored Data:                                                       â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Index snapshots (weekly)                                       â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Document chunks (content-addressed)                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Embedding cache (per document)                                 â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Index metadata (version, stats)                                â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Benefits:                                                          â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Decentralized: Tek nokta arÄ±zasÄ± yok                           â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Content-addressed: AynÄ± dokÃ¼man = aynÄ± hash                    â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Cacheable: CDN-like distribution                               â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Verifiable: Hash ile integrity check                           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Neden FAISS (ChromaDB deÄŸil)?**
| Ã–zellik | FAISS | ChromaDB |
|---------|-------|----------|
| HÄ±z | â­â­â­â­â­ | â­â­â­ |
| Memory efficiency | â­â­â­â­â­ | â­â­â­ |
| GPU support | âœ… Native | âŒ |
| Serialize to IPFS | âœ… Kolay | âš ï¸ Zor |
| Dependencies | Minimal | SQLite, etc. |
| Production-ready | âœ… Meta kullanÄ±yor | âš ï¸ Daha yeni |


---

### Karar 6: Semantic Router - Embedding Model ve Expert Embeddings

**Problem:** Semantic router iÃ§in hangi embedding model kullanÄ±lmalÄ± ve expert embeddings nasÄ±l oluÅŸturulmalÄ±?

**Ã‡Ã¶zÃ¼m:** Hafif multilingual model + pre-computed expert embeddings:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SEMANTIC ROUTER ARCHITECTURE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  EMBEDDING MODEL SEÃ‡Ä°MÄ°:                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  âœ… SELECTED: all-MiniLM-L6-v2                                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Size: 22M params, ~80MB                                        â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Dimension: 384                                                 â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Speed: ~5ms/query (CPU), ~1ms (GPU)                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Quality: Good for short queries                                â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Multilingual: Temel seviye (Ä°ngilizce aÄŸÄ±rlÄ±klÄ±)               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Alternatifler (gerekirse):                                         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ multilingual-e5-small: Daha iyi TÃ¼rkÃ§e, 118M params            â”‚   â”‚
â”‚  â”‚  â””â”€â”€ all-mpnet-base-v2: Daha yÃ¼ksek kalite, 110M params             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  EXPERT EMBEDDING STRATEGY:                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Her expert iÃ§in 15-20 representative query:                        â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  medical_dora:                                                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "hastalÄ±k belirtileri nelerdir"                                â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "bu ilacÄ±n yan etkileri var mÄ±"                                â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "tedavi seÃ§enekleri nelerdir"                                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "what are the symptoms of diabetes"                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "how to treat high blood pressure"                             â”‚   â”‚
â”‚  â”‚  â””â”€â”€ ... (15-20 total)                                              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  coding_dora:                                                       â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "Python'da liste nasÄ±l sÄ±ralanÄ±r"                              â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "JavaScript async await kullanÄ±mÄ±"                             â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "how to implement binary search"                               â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "fix null pointer exception"                                   â”‚   â”‚
â”‚  â”‚  â””â”€â”€ ... (15-20 total)                                              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  turkish_dora:                                                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "TÃ¼rkÃ§e dilbilgisi kurallarÄ±"                                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "TÃ¼rk kÃ¼ltÃ¼rÃ¼ ve gelenekleri"                                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ "Ä°stanbul'un tarihi yerleri"                                   â”‚   â”‚
â”‚  â”‚  â””â”€â”€ ... (15-20 total)                                              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  SIMILARITY CALCULATION:                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Method: Max Pooling (Simple & Effective)                           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  score(query, expert) = max(                                        â”‚   â”‚
â”‚  â”‚      cosine_sim(query_embed, expert_embed_i)                        â”‚   â”‚
â”‚  â”‚      for expert_embed_i in expert_embeddings                        â”‚   â”‚
â”‚  â”‚  )                                                                  â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Neden Max Pooling?                                                 â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ En yakÄ±n Ã¶rneÄŸi yakalar                                        â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Outlier'lara dayanÄ±klÄ±                                         â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Hesaplama basit                                                â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  STORAGE & CACHING:                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Expert Embeddings:                                                 â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Pre-computed at build time                                     â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Stored as .npy files (~50KB per expert)                        â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Loaded to RAM at startup                                       â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Version controlled with expert adapters                        â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Embedding Model:                                                   â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Loaded once at startup (~80MB)                                 â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Cached in RAM                                                  â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Optional: GPU acceleration                                     â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Uygulama:**
```python
class SemanticRouter:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)
        self.expert_embeddings = self._load_expert_embeddings()
    
    def _load_expert_embeddings(self) -> Dict[str, np.ndarray]:
        """Load pre-computed expert embeddings."""
        embeddings = {}
        for expert_id in EXPERT_IDS:
            path = f"router/expert_embeddings/{expert_id}.npy"
            embeddings[expert_id] = np.load(path)
        return embeddings
    
    def route(self, query: str) -> List[ExpertScore]:
        # Embed query
        query_embed = self.model.encode(query, normalize_embeddings=True)
        
        # Calculate similarity with each expert
        scores = []
        for expert_id, expert_embeds in self.expert_embeddings.items():
            # Max pooling over expert embeddings
            similarities = np.dot(expert_embeds, query_embed)
            max_sim = float(np.max(similarities))
            scores.append(ExpertScore(expert_id, max_sim, "semantic"))
        
        return sorted(scores, key=lambda x: x.score, reverse=True)
```

**Expert Embedding Generation Script:**
```python
# scripts/generate_expert_embeddings.py
def generate_expert_embeddings():
    model = SentenceTransformer("all-MiniLM-L6-v2")
    
    expert_queries = {
        "medical_dora": [
            "hastalÄ±k belirtileri nelerdir",
            "bu ilacÄ±n yan etkileri",
            "tedavi seÃ§enekleri",
            # ... 15-20 queries
        ],
        "coding_dora": [
            "Python'da liste nasÄ±l sÄ±ralanÄ±r",
            "JavaScript async await",
            "how to implement binary search",
            # ... 15-20 queries
        ],
        # ... other experts
    }
    
    for expert_id, queries in expert_queries.items():
        embeddings = model.encode(queries, normalize_embeddings=True)
        np.save(f"router/expert_embeddings/{expert_id}.npy", embeddings)
```

---

## Ana Mimari

### 1. BitNet Base Model

BitNet, Microsoft Research tarafÄ±ndan geliÅŸtirilen 1.58-bit quantized model mimarisidir.
AÄŸÄ±rlÄ±klar sadece {-1, 0, +1} deÄŸerlerini alÄ±r.

**AvantajlarÄ±:**
- 10x daha az bellek (FP16'ya gÃ¶re)
- HÄ±zlÄ± inference (Ã§arpma yerine toplama/Ã§Ä±karma)
- Enerji verimli (mobil/edge cihazlarda Ã§alÄ±ÅŸabilir)
- Deterministic (blockchain verification iÃ§in kritik)

**R3MES'te KullanÄ±mÄ±:**
```
BitNet Base Model (Frozen)
â”œâ”€â”€ TÃ¼m kullanÄ±cÄ±larda aynÄ±
â”œâ”€â”€ IPFS'te saklanÄ±r (tek hash)
â”œâ”€â”€ GÃ¼ncellenmez (immutable)
â””â”€â”€ Sadece inference iÃ§in kullanÄ±lÄ±r
```

---

### 2. DoRA Expert System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DoRA EXPERT REGISTRY                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  DOMAIN EXPERTS (UzmanlÄ±k AlanlarÄ±)                            â”‚
â”‚  â”œâ”€â”€ medical_dora      - TÄ±p, saÄŸlÄ±k, hastalÄ±klar              â”‚
â”‚  â”œâ”€â”€ legal_dora        - Hukuk, kanunlar, davalar              â”‚
â”‚  â”œâ”€â”€ coding_dora       - Programlama, debugging                â”‚
â”‚  â”œâ”€â”€ finance_dora      - Finans, yatÄ±rÄ±m, ekonomi              â”‚
â”‚  â”œâ”€â”€ science_dora      - Bilim, fizik, kimya, biyoloji         â”‚
â”‚  â”œâ”€â”€ history_dora      - Tarih, olaylar, kiÅŸiler               â”‚
â”‚  â””â”€â”€ education_dora    - EÄŸitim, Ã¶ÄŸretim, pedagoji             â”‚
â”‚                                                                 â”‚
â”‚  LANGUAGE EXPERTS (Dil AdaptÃ¶rleri)                            â”‚
â”‚  â”œâ”€â”€ turkish_dora      - TÃ¼rkÃ§e dil ve kÃ¼ltÃ¼r                  â”‚
â”‚  â”œâ”€â”€ german_dora       - Almanca                               â”‚
â”‚  â”œâ”€â”€ french_dora       - FransÄ±zca                             â”‚
â”‚  â”œâ”€â”€ spanish_dora      - Ä°spanyolca                            â”‚
â”‚  â”œâ”€â”€ arabic_dora       - ArapÃ§a                                â”‚
â”‚  â””â”€â”€ chinese_dora      - Ã‡ince                                 â”‚
â”‚                                                                 â”‚
â”‚  TASK EXPERTS (GÃ¶rev AdaptÃ¶rleri)                              â”‚
â”‚  â”œâ”€â”€ summarization_dora - Ã–zetleme                             â”‚
â”‚  â”œâ”€â”€ translation_dora   - Ã‡eviri                               â”‚
â”‚  â”œâ”€â”€ qa_dora           - Soru-Cevap                            â”‚
â”‚  â”œâ”€â”€ creative_dora     - YaratÄ±cÄ± yazarlÄ±k                     â”‚
â”‚  â””â”€â”€ analysis_dora     - Analiz ve deÄŸerlendirme               â”‚
â”‚                                                                 â”‚
â”‚  GENERAL (Fallback - sadece gerektiÄŸinde)                      â”‚
â”‚  â””â”€â”€ general_dora      - Genel amaÃ§lÄ±                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3. DoRA Router System (Hybrid)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      HYBRID DoRA ROUTER SYSTEM                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  INPUT: User Query                                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    STAGE 1: KEYWORD ROUTER                           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Regex-based pattern matching                                   â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Domain detection: "hastalÄ±k" â†’ medical_dora                    â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Language detection: "merhaba" â†’ turkish_dora                   â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Task detection: "Ã¶zetle" â†’ summarization_dora                  â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Latency: <1ms                                                  â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Output: [(expert_id, confidence), ...]                             â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚  IF max_confidence >= 0.85:                                  â”‚    â”‚   â”‚
â”‚  â”‚  â”‚      â†’ FAST PATH: Skip semantic router                       â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  ELSE:                                                       â”‚    â”‚   â”‚
â”‚  â”‚  â”‚      â†’ Continue to Stage 2                                   â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼ (confidence < 0.85)                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    STAGE 2: SEMANTIC ROUTER                          â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Model: all-MiniLM-L6-v2 (22M params)                           â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Query â†’ 384-dim embedding                                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Cosine similarity with expert embeddings                       â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Max pooling over 15-20 representative queries per expert       â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Latency: ~10-15ms                                              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Output: [(expert_id, similarity_score), ...]                       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    STAGE 3: SCORE FUSION                             â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  final_score = 0.3 Ã— keyword_score + 0.7 Ã— semantic_score           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Sorted by final_score descending                                   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    STAGE 4: VRAM-ADAPTIVE GATING                     â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  VRAM < 8GB  â†’ Top-1 (tek expert)                                   â”‚   â”‚
â”‚  â”‚  VRAM 8-16GB â†’ Top-2 (2 expert)                                     â”‚   â”‚
â”‚  â”‚  VRAM > 16GB â†’ Top-3 (max 3 expert)                                 â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Fallback: general_dora if max_score < 0.5                          â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚  OUTPUT: Selected DoRA Expert(s) + Weights                                  â”‚
â”‚          [("medical_dora", 0.65), ("turkish_dora", 0.35)]                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Latency Breakdown:**
| Senaryo | Keyword | Semantic | Fusion | Gating | Total |
|---------|---------|----------|--------|--------|-------|
| Fast Path (conf >= 0.85) | <1ms | SKIP | - | <1ms | **~1-2ms** |
| Full Pipeline | <1ms | ~10ms | <1ms | <1ms | **~12-15ms** |

---

### 4. RAG Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            RAG PIPELINE                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  DOCUMENT INGESTION:                                                       â”‚
â”‚  â”œâ”€â”€ Sources: Web, API feeds, User uploads, IPFS                           â”‚
â”‚  â”œâ”€â”€ Processing: Extract â†’ Chunk (512 tokens) â†’ Embed                      â”‚
â”‚  â””â”€â”€ Storage: FAISS index + IPFS documents                                 â”‚
â”‚                                                                             â”‚
â”‚  VECTOR STORE (FAISS):                                                     â”‚
â”‚  â”œâ”€â”€ general_index    - Genel bilgi                                        â”‚
â”‚  â”œâ”€â”€ news_index       - GÃ¼ncel haberler (TTL: 7 gÃ¼n)                       â”‚
â”‚  â”œâ”€â”€ medical_index    - TÄ±bbi bilgiler                                     â”‚
â”‚  â”œâ”€â”€ legal_index      - Hukuki dokÃ¼manlar                                  â”‚
â”‚  â”œâ”€â”€ code_index       - Kod Ã¶rnekleri, dokÃ¼mantasyon                       â”‚
â”‚  â””â”€â”€ user_index       - KullanÄ±cÄ± Ã¶zel dokÃ¼manlarÄ±                         â”‚
â”‚                                                                             â”‚
â”‚  RETRIEVAL:                                                                â”‚
â”‚  â”œâ”€â”€ Dense retrieval (embedding similarity)                                â”‚
â”‚  â”œâ”€â”€ Sparse retrieval (BM25)                                               â”‚
â”‚  â”œâ”€â”€ Hybrid (RRF - Reciprocal Rank Fusion)                                 â”‚
â”‚  â””â”€â”€ Re-ranking (cross-encoder, optional)                                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


---

### 5. Full Inference Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SERVING NODE PIPELINE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  User Request                                                               â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. QUERY ANALYSIS                                                    â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ Tokenization                                                 â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ Language detection                                           â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ Intent classification                                        â”‚   â”‚
â”‚  â”‚    â””â”€â”€ Query embedding                                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 2. PARALLEL PROCESSING                                               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚
â”‚  â”‚    â”‚ DoRA Router  â”‚    â”‚ RAG Retrievalâ”‚    â”‚ Cache Check  â”‚         â”‚   â”‚
â”‚  â”‚    â”‚              â”‚    â”‚              â”‚    â”‚              â”‚         â”‚   â”‚
â”‚  â”‚    â”‚ Select       â”‚    â”‚ Fetch        â”‚    â”‚ Preload      â”‚         â”‚   â”‚
â”‚  â”‚    â”‚ experts      â”‚    â”‚ documents    â”‚    â”‚ adapters     â”‚         â”‚   â”‚
â”‚  â”‚    â”‚ (Top-K)      â”‚    â”‚ (Top-5)      â”‚    â”‚ (async)      â”‚         â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚  â”‚           â”‚                   â”‚                   â”‚                  â”‚   â”‚
â”‚  â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚  â”‚                               â–¼                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 3. INFERENCE (Backend Abstraction)                                   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚    Input: [System Prompt] + [RAG Context] + [User Query]            â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚    â”‚  Backend.inference(                                          â”‚  â”‚   â”‚
â”‚  â”‚    â”‚      input_ids,                                              â”‚  â”‚   â”‚
â”‚  â”‚    â”‚      adapters=["medical_dora", "turkish_dora"],              â”‚  â”‚   â”‚
â”‚  â”‚    â”‚      weights=[0.85, 0.72]                                    â”‚  â”‚   â”‚
â”‚  â”‚    â”‚  )                                                           â”‚  â”‚   â”‚
â”‚  â”‚    â”‚                                                              â”‚  â”‚   â”‚
â”‚  â”‚    â”‚  Internally:                                                 â”‚  â”‚   â”‚
â”‚  â”‚    â”‚  output = bitnet(x)                                         â”‚  â”‚   â”‚
â”‚  â”‚    â”‚         + 0.85 Ã— medical_dora(x)                            â”‚  â”‚   â”‚
â”‚  â”‚    â”‚         + 0.72 Ã— turkish_dora(x)                            â”‚  â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 4. POST-PROCESSING                                                   â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ Token decoding                                               â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ Safety filtering                                             â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ Citation injection (RAG sources)                             â”‚   â”‚
â”‚  â”‚    â””â”€â”€ Response formatting                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  Response to User                                                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Dosya YapÄ±sÄ± (GÃ¼ncellenmiÅŸ)

```
miner-engine/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ bitlinear.py              # âœ… Mevcut - BitNet layer
â”‚   â”œâ”€â”€ dora.py                   # âœ… TAMAMLANDI - Custom DoRA layer
â”‚   â”œâ”€â”€ inference_backend.py      # âœ… TAMAMLANDI - Backend abstraction
â”‚   â”œâ”€â”€ backends/
â”‚   â”‚   â”œâ”€â”€ __init__.py           # âœ… TAMAMLANDI
â”‚   â”‚   â”œâ”€â”€ pytorch_backend.py    # âœ… TAMAMLANDI - Phase 1 backend
â”‚   â”‚   â”œâ”€â”€ triton_backend.py     # ğŸ“… Phase 2 backend (placeholder)
â”‚   â”‚   â””â”€â”€ bitnet_cpp_backend.py # ğŸ“… Phase 3 backend (placeholder)
â”‚   â””â”€â”€ trainer.py                # âœ… Mevcut - DoRA training eklenecek
â”‚
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ __init__.py               # âœ… TAMAMLANDI
â”‚   â”œâ”€â”€ tiered_cache.py           # âœ… TAMAMLANDI - 3-tier caching
â”‚   â”œâ”€â”€ vram_manager.py           # âœ… TAMAMLANDI - VRAM allocation
â”‚   â””â”€â”€ predictive_loader.py      # ğŸ“… Async preloading (MVP'de kapalÄ±)
â”‚
â”œâ”€â”€ router/
â”‚   â”œâ”€â”€ __init__.py               # âœ… TAMAMLANDI
â”‚   â”œâ”€â”€ keyword_router.py         # âœ… TAMAMLANDI - Rule-based routing
â”‚   â”œâ”€â”€ semantic_router.py        # âœ… TAMAMLANDI - Embedding-based routing
â”‚   â”œâ”€â”€ hybrid_router.py          # âœ… TAMAMLANDI - Orchestrator
â”‚   â”œâ”€â”€ vram_adaptive_gating.py   # âœ… TAMAMLANDI - VRAM-based Top-K
â”‚   â””â”€â”€ expert_embeddings/        # âœ… TAMAMLANDI - Pre-computed embeddings
â”‚       â”œâ”€â”€ medical_dora.npy
â”‚       â”œâ”€â”€ coding_dora.npy
â”‚       â”œâ”€â”€ turkish_dora.npy
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ __init__.py               # âœ… TAMAMLANDI
â”‚   â”œâ”€â”€ faiss_store.py            # âœ… TAMAMLANDI - FAISS wrapper
â”‚   â”œâ”€â”€ embedder.py               # âœ… TAMAMLANDI - Document/query embedding
â”‚   â”œâ”€â”€ retriever.py              # âœ… TAMAMLANDI - Hybrid retrieval
â”‚   â”œâ”€â”€ reranker.py               # ğŸ“… Cross-encoder reranking
â”‚   â”œâ”€â”€ document_processor.py     # ğŸ†• Chunking, extraction
â”‚   â””â”€â”€ index_manager.py          # ğŸ†• Index lifecycle management
â”‚
â”œâ”€â”€ r3mes/
â”‚   â”œâ”€â”€ miner/
â”‚   â”‚   â”œâ”€â”€ engine.py             # âœ… Mevcut - DoRA training eklenecek
â”‚   â”‚   â”œâ”€â”€ lora_manager.py       # âœ… Mevcut - DoRA manager olacak
â”‚   â”‚   â””â”€â”€ dora_trainer.py       # ğŸ†• DoRA-specific training
â”‚   â”‚
â”‚   â”œâ”€â”€ serving/
â”‚   â”‚   â”œâ”€â”€ __init__.py           # âœ… TAMAMLANDI - Export'lar
â”‚   â”‚   â”œâ”€â”€ engine.py             # âœ… Mevcut - Blockchain entegrasyonu
â”‚   â”‚   â””â”€â”€ inference_pipeline.py # âœ… TAMAMLANDI - Full inference pipeline
â”‚   â”‚
â”‚   â””â”€â”€ proposer/
â”‚       â”œâ”€â”€ aggregator.py         # âœ… Mevcut
â”‚       â””â”€â”€ dora_aggregator.py    # ğŸ†• DoRA-specific aggregation
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_dora.py              # âœ… TAMAMLANDI - 19 test
    â”œâ”€â”€ test_inference_backend.py # âœ… TAMAMLANDI - 15 test
    â”œâ”€â”€ test_cache.py             # âœ… TAMAMLANDI - 19 test
    â”œâ”€â”€ test_router.py            # âœ… TAMAMLANDI - 22 test
    â”œâ”€â”€ test_semantic_router.py   # âœ… TAMAMLANDI - 19 test
    â”œâ”€â”€ test_hybrid_router.py     # âœ… TAMAMLANDI - 19 test
    â”œâ”€â”€ test_rag.py               # âœ… TAMAMLANDI - 17 test
    â”œâ”€â”€ test_rag_embedder.py      # âœ… TAMAMLANDI - 15 test
    â”œâ”€â”€ test_rag_retriever.py     # âœ… TAMAMLANDI - 17 test
    â”œâ”€â”€ test_inference_pipeline.py # âœ… TAMAMLANDI - 27 test
    â””â”€â”€ test_integration_pipeline.py # âœ… TAMAMLANDI - 26 test (YENÄ°)
```

**Test Durumu:** 215 test geÃ§ti âœ…

---

## KonfigÃ¼rasyon

```yaml
# config/dora_config.yaml

# DoRA Settings
dora:
  default_rank: 16
  default_alpha: 32
  
# Hybrid Router Settings (Karar 1 + Karar 6)
router:
  strategy: "hybrid"  # keyword, semantic, hybrid
  
  # Keyword Router
  keyword:
    confidence_threshold: 0.3
    
  # Semantic Router
  semantic:
    model: "all-MiniLM-L6-v2"  # 22M params, 384 dim
    device: "cpu"  # cpu, cuda
    embeddings_dir: "router/expert_embeddings"
    
  # Hybrid Fusion
  hybrid:
    keyword_weight: 0.3
    semantic_weight: 0.7
    fast_path_threshold: 0.85  # Skip semantic if keyword conf >= this
    
  # VRAM-Adaptive Gating
  gating:
    vram_8gb_max_experts: 1
    vram_16gb_max_experts: 2
    vram_24gb_max_experts: 3
    general_fallback_threshold: 0.5

# Cache Settings (Karar 2)
cache:
  tier1_vram:
    preload: ["turkish_dora", "general_dora"]
    max_adapters: 4
  tier2_ram:
    max_adapters: 20
    eviction_policy: "lru"
  tier3_disk:
    cache_dir: ".r3mes/dora_cache"
    max_size_gb: 10
  predictive_loading: false  # MVP'de kapalÄ±

# Backend Settings (Karar 3)
inference:
  backend: "auto"  # auto, pytorch, triton, bitnet_cpp
  fallback_backend: "pytorch"

# RAG Settings (Karar 5)
rag:
  vector_store: "faiss"
  embedding_model: "all-MiniLM-L6-v2"
  chunk_size: 512
  chunk_overlap: 50
  top_k: 5
  hybrid_search: true
  rerank: false  # MVP'de kapalÄ±
  
  # Serving Node (merkezi)
  serving_node:
    indices: ["general", "news", "medical", "code"]
    update_interval_hours: 6
  
  # Miner Node (yerel, opsiyonel)
  miner_node:
    enabled: false
    sync_from_serving: true
    local_indices: ["user"]
```

---

## Implementation Roadmap

### Tamamlanan (âœ…)
| # | BileÅŸen | Dosya | Durum | Test |
|---|---------|-------|-------|------|
| 1 | DoRA Layer | `core/dora.py` | âœ… TamamlandÄ± | 19 test |
| 2 | Backend Abstraction | `core/inference_backend.py` | âœ… TamamlandÄ± | 15 test |
| 3 | PyTorch Backend | `core/backends/pytorch_backend.py` | âœ… TamamlandÄ± | - |
| 4 | Tiered Cache | `cache/tiered_cache.py` | âœ… TamamlandÄ± | 19 test |
| 5 | VRAM Manager | `cache/vram_manager.py` | âœ… TamamlandÄ± | - |
| 6 | Keyword Router | `router/keyword_router.py` | âœ… TamamlandÄ± | 22 test |
| 7 | Semantic Router | `router/semantic_router.py` | âœ… TamamlandÄ± | 19 test |
| 8 | Hybrid Router | `router/hybrid_router.py` | âœ… TamamlandÄ± | 19 test |
| 9 | VRAM-Adaptive Gating | `router/vram_adaptive_gating.py` | âœ… TamamlandÄ± | - |
| 10 | FAISS Store | `rag/faiss_store.py` | âœ… TamamlandÄ± | 17 test |
| 11 | RAG Embedder | `rag/embedder.py` | âœ… TamamlandÄ± | 15 test |
| 12 | RAG Retriever | `rag/retriever.py` | âœ… TamamlandÄ± | 17 test |
| 13 | **Inference Pipeline** | `r3mes/serving/inference_pipeline.py` | âœ… TamamlandÄ± | 27 test |
| 14 | **Integration Tests** | `tests/test_integration_pipeline.py` | âœ… TamamlandÄ± | 26 test |

**Toplam: 215 test geÃ§ti âœ…**

### SÄ±radaki (ğŸ†•)
| # | BileÅŸen | Dosya | Tahmini SÃ¼re | BaÄŸÄ±mlÄ±lÄ±k |
|---|---------|-------|--------------|------------|
| 15 | End-to-End Tests | `tests/test_e2e.py` | 3-4 saat | GerÃ§ek model |
| 16 | Performance Benchmarks | `benchmarks/` | 2-3 saat | Pipeline |
| 17 | Production Deployment | ServingEngine entegrasyonu | 4-5 saat | Pipeline |

### Gelecek (ğŸ“…)
| # | BileÅŸen | Dosya | Hedef |
|---|---------|-------|-------|
| 18 | Triton Backend | `core/backends/triton_backend.py` | Phase 2 (3-6 ay) |
| 19 | BitNet-cpp Backend | `core/backends/bitnet_cpp_backend.py` | Phase 3 (6-12 ay) |
| 20 | Predictive Loader | `cache/predictive_loader.py` | Post-MVP |
| 21 | Cross-encoder Reranker | `rag/reranker.py` | Post-MVP |

**Toplam Ä°lerleme:** 14/14 temel bileÅŸen tamamlandÄ± (100%)

---

## Ã–rnek KullanÄ±m SenaryolarÄ±

### Senaryo 1: TÄ±bbi Soru (TÃ¼rkÃ§e, 8GB VRAM)
```
User: "Diyabet hastalarÄ±nda insÃ¼lin direnci nasÄ±l tedavi edilir?"

1. Router:
   - Keyword: "diyabet", "insÃ¼lin", "tedavi" â†’ medical_dora (0.9)
   - Language: TÃ¼rkÃ§e â†’ turkish_dora (0.8)
   - VRAM: 8GB â†’ Top-1 Gating â†’ medical_dora seÃ§ilir

2. Cache:
   - medical_dora Tier 2'de (RAM) â†’ Tier 1'e (VRAM) promote
   - Predictive: turkish_dora arka planda yÃ¼klenir

3. RAG:
   - medical_index'ten ilgili dokÃ¼manlar
   - "Ä°nsÃ¼lin direnci tedavisinde metformin..."

4. Inference:
   - BitNet + medical_dora(1.0)
   - RAG context ile zenginleÅŸtirilmiÅŸ cevap
```

### Senaryo 2: Kod Sorusu (Ä°ngilizce, 24GB VRAM)
```
User: "How to implement a binary search tree in Python?"

1. Router:
   - Keyword: "implement", "Python" â†’ coding_dora (0.95)
   - Language: English â†’ (no language adapter)
   - VRAM: 24GB â†’ Top-3 Gating â†’ coding_dora + general_dora

2. Cache:
   - coding_dora Tier 1'de (VRAM) â†’ Hemen kullan
   - general_dora Tier 1'de (VRAM) â†’ Hemen kullan

3. RAG:
   - code_index'ten Python BST Ã¶rnekleri

4. Inference:
   - BitNet + coding_dora(0.95) + general_dora(0.3)
   - Kod Ã¶rnekleri ile cevap
```


---

## Inference Pipeline (YENÄ° - v3.3)

### Pipeline Mimarisi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INFERENCE PIPELINE (v3.3)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  User Query                                                                 â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STAGE 1: RAG CONTEXT RETRIEVAL                                       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Query embedding (all-MiniLM-L6-v2)                             â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ FAISS similarity search (top-k=3)                              â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Context augmentation                                           â”‚   â”‚
â”‚  â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”‚  Template:                                                        â”‚   â”‚
â”‚  â”‚  â”‚  "Context:\n{retrieved_docs}\n\nQuery: {user_query}"             â”‚   â”‚
â”‚  â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Latency: ~10-20ms                                              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Skip if: enable_rag=False or skip_rag=True                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STAGE 2: EXPERT ROUTING (HybridRouter)                               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Keyword Router (<1ms)                                          â”‚   â”‚
â”‚  â”‚  â”‚   â””â”€â”€ IF confidence >= 0.85 â†’ FAST PATH                          â”‚   â”‚
â”‚  â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Semantic Router (~10ms) - if not fast path                     â”‚   â”‚
â”‚  â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Score Fusion                                                   â”‚   â”‚
â”‚  â”‚  â”‚   â””â”€â”€ 0.3 Ã— keyword + 0.7 Ã— semantic                             â”‚   â”‚
â”‚  â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â””â”€â”€ VRAM-Adaptive Gating                                           â”‚   â”‚
â”‚  â”‚      â”œâ”€â”€ <8GB: Top-1                                                â”‚   â”‚
â”‚  â”‚      â”œâ”€â”€ 8-16GB: Top-2                                              â”‚   â”‚
â”‚  â”‚      â””â”€â”€ >16GB: Top-3                                               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Output: [(expert_id, weight), ...]                                 â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Skip if: force_experts provided                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STAGE 3: ADAPTER LOADING (TieredCache)                               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  For each selected expert:                                          â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Check VRAM cache (Tier 1) â†’ 0ms                                â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Check RAM cache (Tier 2) â†’ ~5ms                                â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Load from Disk (Tier 3) â†’ ~50-100ms                            â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Promote to higher tier if space available                      â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Metrics: cache_hits, cache_misses                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STAGE 4: INFERENCE EXECUTION (InferenceBackend)                      â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Backend.inference(                                                 â”‚   â”‚
â”‚  â”‚      input_ids,                                                     â”‚   â”‚
â”‚  â”‚      adapter_ids=["medical_dora", "turkish_dora"],                  â”‚   â”‚
â”‚  â”‚      adapter_weights=[0.65, 0.35]                                   â”‚   â”‚
â”‚  â”‚  )                                                                  â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Internally:                                                        â”‚   â”‚
â”‚  â”‚  output = bitnet(x)                                                 â”‚   â”‚
â”‚  â”‚         + 0.65 Ã— medical_dora(x)                                    â”‚   â”‚
â”‚  â”‚         + 0.35 Ã— turkish_dora(x)                                    â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Backend Priority: BitNet-cpp > Triton > PyTorch                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  OUTPUT: PipelineResult                                                     â”‚
â”‚  â”œâ”€â”€ output: torch.Tensor                                                  â”‚
â”‚  â”œâ”€â”€ metrics: PipelineMetrics                                              â”‚
â”‚  â”œâ”€â”€ experts_used: [(expert_id, weight), ...]                              â”‚
â”‚  â”œâ”€â”€ rag_context: Optional[str]                                            â”‚
â”‚  â””â”€â”€ success: bool                                                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline KullanÄ±mÄ±

```python
from r3mes.serving import (
    InferencePipeline,
    PipelineConfig,
    create_pipeline,
)

# 1. Basit kullanÄ±m
pipeline = create_pipeline(enable_rag=True)
await pipeline.initialize()
await pipeline.load_model("path/to/model")

result = await pipeline.run("Diyabet tedavisi hakkÄ±nda bilgi ver")
print(result.output)
print(result.experts_used)  # [("medical_dora", 0.8), ("turkish_dora", 0.2)]
print(result.metrics.total_time_ms)  # ~50ms

# 2. Ã–zelleÅŸtirilmiÅŸ konfigÃ¼rasyon
config = PipelineConfig(
    enable_rag=True,
    rag_top_k=5,
    router_strategy="hybrid",
    keyword_weight=0.3,
    semantic_weight=0.7,
    fast_path_threshold=0.85,
    vram_capacity_mb=4096,
)

pipeline = InferencePipeline(config=config)
await pipeline.initialize()

# 3. RAG dokÃ¼manlarÄ± ekleme
pipeline.add_rag_document(
    doc_id="med_001",
    content="Diyabet tedavisinde metformin ilk tercih ilaÃ§tÄ±r...",
    metadata={"domain": "medical", "source": "guidelines"}
)

# 4. Batch inference
queries = ["Soru 1", "Soru 2", "Soru 3"]
results = await pipeline.run_batch(queries)

# 5. Streaming inference
async for token in pipeline.run_streaming("Uzun bir cevap ver"):
    print(token, end="", flush=True)

# 6. Force specific experts
result = await pipeline.run(
    "Test query",
    force_experts=["coding_dora", "general_dora"]
)

# 7. Ä°statistikler
stats = pipeline.get_stats()
print(stats["router"]["fast_path_rate"])  # 0.65
print(stats["cache"]["vram"]["utilization"])  # 0.75
```

### Pipeline Metrikleri

```python
@dataclass
class PipelineMetrics:
    # Timing
    total_time_ms: float      # Toplam sÃ¼re
    rag_time_ms: float        # RAG retrieval sÃ¼resi
    routing_time_ms: float    # Router sÃ¼resi
    loading_time_ms: float    # Adapter yÃ¼kleme sÃ¼resi
    inference_time_ms: float  # Inference sÃ¼resi
    
    # RAG
    rag_docs_retrieved: int   # Bulunan dokÃ¼man sayÄ±sÄ±
    rag_context_length: int   # Context karakter sayÄ±sÄ±
    
    # Routing
    used_fast_path: bool      # Fast path kullanÄ±ldÄ± mÄ±
    keyword_confidence: float # Keyword router gÃ¼veni
    
    # Cache
    adapters_loaded: List[str]  # YÃ¼klenen adapter'lar
    cache_hits: int           # Cache hit sayÄ±sÄ±
    cache_misses: int         # Cache miss sayÄ±sÄ±
    
    # Inference
    tokens_generated: int     # Ãœretilen token sayÄ±sÄ±
    backend_used: str         # KullanÄ±lan backend
```

### Dosya Konumu

```
miner-engine/r3mes/serving/
â”œâ”€â”€ __init__.py              # Export'lar
â”œâ”€â”€ engine.py                # Mevcut ServingEngine (blockchain entegrasyonu)
â””â”€â”€ inference_pipeline.py    # YENÄ° - Full inference pipeline
```

---

## Sonraki AdÄ±mlar

### Tamamlanan (v3.4)
- âœ… Inference Pipeline (`r3mes/serving/inference_pipeline.py`) - 27 test
- âœ… Integration Tests (`tests/test_integration_pipeline.py`) - 26 test
- âœ… Toplam 215 test geÃ§ti
- âœ… **14/14 temel bileÅŸen tamamlandÄ± (100%)**

### SÄ±radaki
1. **End-to-End Tests** - GerÃ§ek model ile test
2. **Performance Benchmarks** - Latency ve throughput Ã¶lÃ§Ã¼mÃ¼
3. **Production Deployment** - ServingEngine entegrasyonu

---

*Son gÃ¼ncelleme: Ocak 2026 - v3.4 (Integration Tests)*
