from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from dotenv import load_dotenv
import os
import time
import requests

load_dotenv()

app = FastAPI(title="R3MES Node API")

# --- Veri Modelleri ---
class GenerateRequest(BaseModel):
    prompt: str
    max_length: int = 50

# --- YardÄ±mcÄ± Fonksiyonlar ---
def get_latest_block():
    try:
        rpc_url = os.getenv("RPC_URL", "http://localhost:26657")
        response = requests.get(f"{rpc_url}/status", timeout=2)
        data = response.json()
        return data['result']['sync_info']['latest_block_height']
    except requests.exceptions.RequestException as e:
        # Log connection errors for debugging
        print(f"Warning: Could not fetch latest block: {e}")
        return "Unknown"
    except (KeyError, ValueError) as e:
        # Log parsing errors
        print(f"Warning: Invalid response format from RPC: {e}")
        return "Unknown"

@app.get("/")
def read_root():
    return {"status": "R3MES Node is Active", "service": "Backend v1.0"}

@app.get("/chain/status")
def chain_status():
    try:
        rpc_url = os.getenv("RPC_URL", "http://localhost:26657")
        response = requests.get(f"{rpc_url}/status", timeout=2)
        data = response.json()
        latest_block = data['result']['sync_info']['latest_block_height']
        return {"chain_connected": True, "latest_block": latest_block}
    except Exception as e:
        return {"chain_connected": False, "error": str(e)}

# --- YENÄ°: AI Ãœretim Endpoint'i ---
@app.post("/generate")
def generate_text(request: GenerateRequest):
    # 1. Model KontrolÃ¼
    model_path = os.getenv("MODEL_PATH", "./models/llama-3-8b")
    config_file = os.path.join(model_path, "config.json")
    
    if not os.path.exists(config_file):
        raise HTTPException(status_code=500, detail="Model yÃ¼klenemedi! Dosyalar eksik.")

    # 2. Zincir Durumunu Al (Proof of Generation iÃ§in gerekli olacak)
    block_height = get_latest_block()

    # 3. SimÃ¼le EdilmiÅŸ "DÃ¼ÅŸÃ¼nme" SÃ¼reci (AI burada Ã§alÄ±ÅŸacak)
    print(f"ðŸ§  DÃ¼ÅŸÃ¼nÃ¼lÃ¼yor... Prompt: {request.prompt}")
    time.sleep(1) # Ä°ÅŸlem simÃ¼lasyonu

    # 4. Cevap OluÅŸtur
    ai_response = f"R3MES AI (Simulasyon): '{request.prompt}' sorunu aldÄ±m. Åžu an Blok #{block_height} Ã¼zerindeyiz ve sistem sorunsuz Ã§alÄ±ÅŸÄ±yor."

    return {
        "model": "llama-3-8b-simulated",
        "input": request.prompt,
        "output": ai_response,
        "proof": f"generated_at_block_{block_height}"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
