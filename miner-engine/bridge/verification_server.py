"""
Verification Service - gRPC Server
Provides inference service for Go validator nodes to verify miner gradients
"""

import grpc
import logging
import time
from concurrent import futures
from typing import Optional
import os
import sys

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from r3mes.miner.model_loader import load_model_with_enforced_lora
from r3mes.miner.engine import MinerEngine
from r3mes.data.dataset_loader import DatasetLoader
from utils.ipfs_client import IPFSClient
import torch
import tempfile
import shutil
from pathlib import Path

# Import generated protobuf code (will be generated by buf)
# For now, we'll use a placeholder structure
try:
    from bridge.proto.verification_pb2 import (
        RunForwardPassRequest,
        RunForwardPassResponse,
        HealthCheckRequest,
        HealthCheckResponse,
    )
    from bridge.proto.verification_pb2_grpc import (
        VerificationServiceServicer,
        add_VerificationServiceServicer_to_server,
    )
except ImportError:
    # Fallback if protobuf files not generated yet
    logging.warning("Protobuf files not generated. Using placeholder classes.")
    
    class RunForwardPassRequest:
        weights_ipfs_hash: str = ""
        batch_id: str = ""
        model_config_id: int = 0
        dataset_ipfs_hash: str = ""
        timeout_seconds: int = 300
    
    class RunForwardPassResponse:
        loss_int: int = 0
        success: bool = False
        error_message: str = ""
        execution_time_ms: int = 0
        gpu_info: str = ""
    
    class HealthCheckRequest:
        pass
    
    class HealthCheckResponse:
        healthy: bool = False
        version: str = ""
        available_gpus: list = []
        model_loaded: bool = False
    
    class VerificationServiceServicer:
        pass
    
    def add_VerificationServiceServicer_to_server(servicer, server):
        pass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VerificationServiceImpl(VerificationServiceServicer):
    """
    Implementation of the Verification Service
    Handles forward pass requests from Go validator nodes
    """
    
    def __init__(self, base_model_path: str, dataset_path: Optional[str] = None, ipfs_api_url: Optional[str] = None):
        # Normalize paths (resolve relative paths, keep absolute as-is)
        if base_model_path and not Path(base_model_path).is_absolute():
            self.base_model_path = str(Path.cwd() / base_model_path)
        else:
            self.base_model_path = str(Path(base_model_path).resolve()) if base_model_path else base_model_path
        
        if dataset_path and not Path(dataset_path).is_absolute():
            self.dataset_path = str(Path.cwd() / dataset_path)
        else:
            self.dataset_path = str(Path(dataset_path).resolve()) if dataset_path else dataset_path
        self.model = None
        self.dataset_loader = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.gpu_info = self._get_gpu_info()
        
        # Initialize IPFS client for model/dataset retrieval
        # In production, IPFS_API_URL must be set (no localhost fallback)
        is_production = os.getenv("R3MES_ENV", "development").lower() == "production"
        ipfs_api_url_env = ipfs_api_url or os.getenv("IPFS_API_URL")
        if not ipfs_api_url_env:
            if is_production:
                raise ValueError(
                    "IPFS_API_URL environment variable must be set in production. "
                    "Do not use localhost in production."
                )
            # Development fallback
            ipfs_api_url_env = "/ip4/127.0.0.1/tcp/5001"
            logger.warning("IPFS_API_URL not set, using localhost fallback (development only)")
        else:
            # Validate that production doesn't use localhost
            if is_production and ("127.0.0.1" in ipfs_api_url_env or "localhost" in ipfs_api_url_env):
                raise ValueError(
                    f"IPFS_API_URL cannot use localhost in production: {ipfs_api_url_env}"
                )
        self.ipfs_client = IPFSClient(api_url=ipfs_api_url_env)
        self.temp_dir = Path(tempfile.mkdtemp(prefix="r3mes_verification_"))
        logger.info(f"Verification Service initialized on device: {self.device}")
        logger.info(f"GPU Info: {self.gpu_info}")
        logger.info(f"IPFS client initialized: {self.ipfs_client.is_connected()}")
        logger.info(f"Temporary directory: {self.temp_dir}")
    
    def _get_gpu_info(self) -> str:
        """Get GPU information"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            return f"{gpu_name} ({gpu_memory:.1f}GB)"
        return "CPU"
    
    def _load_model_from_ipfs(self, weights_ipfs_hash: str) -> bool:
        """
        Load model weights from IPFS
        
        Args:
            weights_ipfs_hash: IPFS hash of the model weights
            
        Returns:
            True if successful, False otherwise
        """
        try:
            logger.info(f"Loading model from IPFS: {weights_ipfs_hash}")
            
            # Check if IPFS client is connected
            if not self.ipfs_client.is_connected():
                logger.error("IPFS client not connected - cannot load model from IPFS")
                return False
            
            # 1. Download weights from IPFS
            model_data = self.ipfs_client.retrieve_content(weights_ipfs_hash)
            if model_data is None:
                logger.error(f"Failed to retrieve model from IPFS: {weights_ipfs_hash}")
                return False
            
            # 2. Save to temporary directory
            model_dir = self.temp_dir / f"model_{weights_ipfs_hash[:8]}"
            model_dir.mkdir(parents=True, exist_ok=True)
            
            # Save model weights (assuming it's a directory structure or single file)
            # For now, save as pickle or state dict
            model_path = model_dir / "model.pt"
            with open(model_path, "wb") as f:
                f.write(model_data)
            
            logger.info(f"Model downloaded from IPFS to {model_path}")
            
            # 3. Load model with those weights
            # Note: This depends on the model format stored in IPFS
            # For now, we'll use the base model and load adapter if available
            if self.model is None:
                # Try to load as adapter first, fallback to base model
                adapter_path = model_dir if (model_dir / "adapter_config.json").exists() else None
                self.model = load_model_with_enforced_lora(
                    base_model_path=self.base_model_path,
                    adapter_path=str(adapter_path) if adapter_path else None,
                )
                logger.info("Model loaded successfully from IPFS")
            
            return True
        except Exception as e:
            logger.error(f"Failed to load model from IPFS: {e}", exc_info=True)
            return False
    
    def _get_batch_from_dataset(self, batch_id: str, dataset_ipfs_hash: Optional[str] = None):
        """
        Get a batch from the dataset using deterministic batch selection
        
        Args:
            batch_id: Deterministic batch identifier
            dataset_ipfs_hash: IPFS hash of the dataset (optional)
            
        Returns:
            Batch data (tokens, labels, etc.)
        """
        try:
            if self.dataset_loader is None:
                # Load dataset from IPFS if dataset_ipfs_hash is provided
                if dataset_ipfs_hash:
                    logger.info(f"Loading dataset from IPFS: {dataset_ipfs_hash}")
                    
                    # Check if IPFS client is connected
                    if not self.ipfs_client.is_connected():
                        logger.error("IPFS client not connected - cannot load dataset from IPFS")
                        return None
                    
                    # Download dataset from IPFS
                    dataset_data = self.ipfs_client.retrieve_content(dataset_ipfs_hash)
                    if dataset_data is None:
                        logger.error(f"Failed to retrieve dataset from IPFS: {dataset_ipfs_hash}")
                        return None
                    
                    # Save to temporary directory
                    dataset_dir = self.temp_dir / f"dataset_{dataset_ipfs_hash[:8]}"
                    dataset_dir.mkdir(parents=True, exist_ok=True)
                    
                    # Save dataset (assuming it's a directory structure)
                    dataset_path = dataset_dir / "dataset"
                    with open(dataset_path, "wb") as f:
                        f.write(dataset_data)
                    
                    logger.info(f"Dataset downloaded from IPFS to {dataset_path}")
                    self.dataset_loader = DatasetLoader(str(dataset_dir))
                elif self.dataset_path:
                    # Fallback to local dataset path
                    self.dataset_loader = DatasetLoader(self.dataset_path)
                else:
                    logger.warning("No dataset path or IPFS hash provided")
                    return None
            
            # Use batch_id as seed for deterministic batch selection
            # This ensures the same batch_id always returns the same batch
            batch = self.dataset_loader.get_batch_by_id(batch_id)
            return batch
        except Exception as e:
            logger.error(f"Failed to get batch from dataset: {e}", exc_info=True)
            return None
    
    def RunForwardPass(self, request: RunForwardPassRequest, context) -> RunForwardPassResponse:
        """
        Run a forward pass (inference) on a batch to calculate loss
        
        This is the core verification function called by Go validators
        """
        start_time = time.time()
        
        try:
            # 1. Load model weights from IPFS
            if not self._load_model_from_ipfs(request.weights_ipfs_hash):
                return RunForwardPassResponse(
                    success=False,
                    error_message=f"Failed to load model from IPFS: {request.weights_ipfs_hash}",
                    execution_time_ms=int((time.time() - start_time) * 1000),
                )
            
            # 2. Get batch from dataset
            batch = self._get_batch_from_dataset(
                request.batch_id,
                request.dataset_ipfs_hash if request.dataset_ipfs_hash else None,
            )
            
            if batch is None:
                return RunForwardPassResponse(
                    success=False,
                    error_message=f"Failed to get batch: {request.batch_id}",
                    execution_time_ms=int((time.time() - start_time) * 1000),
                )
            
            # 3. Run forward pass
            self.model.eval()
            with torch.no_grad():
                # Prepare batch
                input_ids = batch["input_ids"].to(self.device)
                labels = batch["labels"].to(self.device) if "labels" in batch else None
                
                # Forward pass
                outputs = self.model(input_ids=input_ids, labels=labels)
                
                # Calculate loss
                if hasattr(outputs, "loss"):
                    loss = outputs.loss.item()
                else:
                    # Manual loss calculation if needed
                    logits = outputs.logits
                    loss = torch.nn.functional.cross_entropy(
                        logits.view(-1, logits.size(-1)),
                        labels.view(-1),
                    ).item()
            
            # 4. Convert loss to BitNet integer
            # BitNet uses integer quantization, so we need to convert float loss to int
            loss_int = int(loss * 10000)  # Scale to integer (adjust scale as needed)
            
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            logger.info(
                f"Forward pass completed: loss={loss:.4f}, loss_int={loss_int}, "
                f"time={execution_time_ms}ms"
            )
            
            return RunForwardPassResponse(
                success=True,
                loss_int=loss_int,
                execution_time_ms=execution_time_ms,
                gpu_info=self.gpu_info,
            )
            
        except Exception as e:
            logger.error(f"Error in RunForwardPass: {e}", exc_info=True)
            return RunForwardPassResponse(
                success=False,
                error_message=str(e),
                execution_time_ms=int((time.time() - start_time) * 1000),
            )
    
    def HealthCheck(self, request: HealthCheckRequest, context) -> HealthCheckResponse:
        """Health check endpoint"""
        return HealthCheckResponse(
            healthy=True,
            version="1.0.0",
            available_gpus=[self.gpu_info] if torch.cuda.is_available() else [],
            model_loaded=self.model is not None,
        )


def serve(port: int = 50051, base_model_path: str = "checkpoints/base_model", dataset_path: Optional[str] = None):
    # Normalize base_model_path if provided
    if base_model_path and not Path(base_model_path).is_absolute():
        base_model_path = str(Path.cwd() / base_model_path)
    elif base_model_path:
        base_model_path = str(Path(base_model_path).resolve())
    """
    Start the gRPC verification server
    
    Args:
        port: gRPC server port (default: 50051)
        base_model_path: Path to base model
        dataset_path: Path to dataset (optional)
    """
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    
    verification_service = VerificationServiceImpl(base_model_path, dataset_path)
    add_VerificationServiceServicer_to_server(verification_service, server)
    
    server.add_insecure_port(f"[::]:{port}")
    server.start()
    
    logger.info(f"Verification Service started on port {port}")
    logger.info("Waiting for requests from Go validator nodes...")
    
    try:
        server.wait_for_termination()
    except KeyboardInterrupt:
        logger.info("Shutting down verification service...")
        server.stop(0)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="R3MES Verification Service (gRPC)")
    parser.add_argument(
        "--port",
        type=int,
        default=50051,
        help="gRPC server port (default: 50051)",
    )
    parser.add_argument(
        "--base-model-path",
        type=str,
        default="checkpoints/base_model",
        help="Path to base model",
    )
    parser.add_argument(
        "--dataset-path",
        type=str,
        default=None,
        help="Path to dataset (optional)",
    )
    
    args = parser.parse_args()
    
    serve(
        port=args.port,
        base_model_path=args.base_model_path,
        dataset_path=args.dataset_path,
    )

